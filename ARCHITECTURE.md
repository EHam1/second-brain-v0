# Second Brain Architecture

## üéØ Purpose
This document explains how your semantic memory system works under the hood. Perfect for learning RAG (Retrieval Augmented Generation) concepts with a real, practical application.

---

## üèóÔ∏è High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         USER                                ‚îÇ
‚îÇ                     (CLI Interface)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   MEMORY MANAGER                            ‚îÇ
‚îÇ              (Business Logic Layer)                         ‚îÇ
‚îÇ  ‚Ä¢ Add memory with review step                              ‚îÇ
‚îÇ  ‚Ä¢ Recall with hybrid scoring                               ‚îÇ
‚îÇ  ‚Ä¢ List/Delete operations                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                           ‚îÇ
             ‚ñº                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   EMBEDDINGS ENGINE    ‚îÇ   ‚îÇ    VECTOR STORE (ChromaDB)   ‚îÇ
‚îÇ  sentence-transformers ‚îÇ   ‚îÇ   Persistent Local Storage   ‚îÇ
‚îÇ   (MiniLM-L6-v2)       ‚îÇ   ‚îÇ                              ‚îÇ
‚îÇ                        ‚îÇ   ‚îÇ  Stores:                     ‚îÇ
‚îÇ  Converts text ‚Üí       ‚îÇ   ‚îÇ  ‚Ä¢ Text                      ‚îÇ
‚îÇ  384-dim vector        ‚îÇ   ‚îÇ  ‚Ä¢ Embeddings (vectors)      ‚îÇ
‚îÇ                        ‚îÇ   ‚îÇ  ‚Ä¢ Metadata (timestamp, etc) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìö RAG Concepts Explained

### What is RAG?
**Retrieval Augmented Generation** = Retrieve relevant info + Generate response

For this "Lite" version, we're doing **RAG without the Generation**:
- ‚úÖ **Retrieval**: Find semantically similar memories
- ‚ùå **Generation**: No LLM (future version)

### The Three Core Components

#### 1Ô∏è‚É£ **Embeddings** (The "Understanding" Layer)
Converts text into mathematical vectors that capture meaning.

```python
# Example (simplified):
"passport in blue suitcase"  ‚Üí  [0.23, -0.15, 0.87, ...]  (384 numbers)
"where is my passport?"      ‚Üí  [0.21, -0.13, 0.89, ...]  (similar!)
"what's for dinner?"         ‚Üí  [-0.45, 0.67, -0.12, ...] (different)
```

**Why?** Computers can measure similarity between vectors using math (cosine similarity).

**Model**: `sentence-transformers/all-MiniLM-L6-v2`
- Fast (runs on CPU)
- Small (384 dimensions)
- Accurate enough for our use case
- Fully local (no API calls)

#### 2Ô∏è‚É£ **Vector Store** (The "Memory" Layer)
Stores memories with their embeddings for fast similarity search.

```python
# When you add: "passport in blue suitcase"
{
    "id": "a3f2",
    "text": "passport in blue suitcase",
    "embedding": [0.23, -0.15, 0.87, ...],  # 384 numbers
    "timestamp": "2025-10-13T20:32:00",
    "metadata": {}
}
```

**ChromaDB** handles:
- Storing vectors on disk (`./data/chroma/`)
- Fast similarity search (finds nearest vectors)
- Persistence between runs

#### 3Ô∏è‚É£ **Hybrid Scoring** (The "Smart Recall" Layer)
Combines semantic similarity + recency to find the best answer.

```python
# For each memory, calculate:
similarity_score = cosine_similarity(query_vector, memory_vector)  # 0-1
recency_score = exp(-days_old * decay_rate)  # Newer = higher

final_score = (similarity_score * 0.7) + (recency_score * 0.3)
```

**Example:**
```
Query: "where's my passport?"

Memory 1: "passport in blue suitcase" (30 days ago)
  - Similarity: 0.95 (very relevant)
  - Recency: 0.30 (old)
  - Final: 0.95*0.7 + 0.30*0.3 = 0.755

Memory 2: "passport in red backpack" (1 day ago)
  - Similarity: 0.92 (very relevant)  
  - Recency: 0.95 (recent)
  - Final: 0.92*0.7 + 0.95*0.3 = 0.929 ‚úÖ WINNER
```

---

## üóÇÔ∏è Project Structure

```
second-brain-v0/
‚îú‚îÄ‚îÄ config.py                 # All tunable parameters in one place
‚îÇ                            # (similarity weights, decay rates, thresholds)
‚îÇ
‚îú‚îÄ‚îÄ core/                     # Core RAG components
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py         # Embedding model initialization & encoding
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py       # ChromaDB wrapper (add, search, delete)
‚îÇ   ‚îî‚îÄ‚îÄ memory_manager.py     # Business logic (hybrid scoring, CRUD ops)
‚îÇ
‚îú‚îÄ‚îÄ cli/                      # User interface
‚îÇ   ‚îî‚îÄ‚îÄ brain.py             # Command-line interface
‚îÇ
‚îú‚îÄ‚îÄ tests/                    # Test scenarios
‚îÇ   ‚îî‚îÄ‚îÄ test_memory.py       # Passport example, conflict resolution, etc.
‚îÇ
‚îú‚îÄ‚îÄ data/                     # Auto-created at runtime
‚îÇ   ‚îî‚îÄ‚îÄ chroma/              # Vector database storage (gitignored)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ README.md                 # User guide (how to install & use)
‚îî‚îÄ‚îÄ ARCHITECTURE.md          # This file!
```

---

## üîÑ Data Flow: Adding a Memory

```
1. User Input
   ‚îî‚îÄ> "passport in blue suitcase"

2. Preview & Confirm (CLI)
   ‚îî‚îÄ> Show text, timestamp, ID
   ‚îî‚îÄ> Wait for confirmation

3. Generate Embedding
   ‚îî‚îÄ> embeddings.py: text ‚Üí 384-dim vector

4. Store in Vector DB
   ‚îî‚îÄ> vector_store.py: save to ChromaDB
   ‚îî‚îÄ> Persisted to ./data/chroma/

5. Confirmation
   ‚îî‚îÄ> "‚úì Memory saved! [ID: a3f2]"
```

---

## üîç Data Flow: Recalling a Memory

```
1. User Query
   ‚îî‚îÄ> "where's my passport?"

2. Generate Query Embedding
   ‚îî‚îÄ> embeddings.py: query ‚Üí 384-dim vector

3. Vector Similarity Search
   ‚îî‚îÄ> vector_store.py: find top K similar vectors
   ‚îî‚îÄ> ChromaDB returns top 10 matches

4. Hybrid Scoring
   ‚îî‚îÄ> memory_manager.py: 
       - Calculate recency score for each
       - Combine with similarity score
       - Re-rank results

5. Return Top 3
   ‚îî‚îÄ> Filter by confidence threshold (default: 0.3)
   ‚îî‚îÄ> Format response with timestamps
   ‚îî‚îÄ> "You said: 'passport in red backpack' on Oct 13, 8:15 PM"
```

---

## ‚öôÔ∏è Configuration & Tuning

All tunable parameters live in `config.py`:

```python
# Embedding model
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# Hybrid scoring weights
SIMILARITY_WEIGHT = 0.7  # How much semantic similarity matters
RECENCY_WEIGHT = 0.3     # How much recency matters

# Recency decay
RECENCY_DECAY_RATE = 0.1  # How fast old memories fade (higher = faster)

# Search parameters
TOP_K_RETRIEVAL = 10      # How many candidates to retrieve from vector DB
TOP_N_RESULTS = 3         # How many to show to user
CONFIDENCE_THRESHOLD = 0.3  # Minimum score to return

# Storage
DATA_DIR = "./data/chroma"
COLLECTION_NAME = "memories"
```

**What happens if you change these?**

- ‚Üë `RECENCY_WEIGHT` ‚Üí More emphasis on recent memories
- ‚Üë `RECENCY_DECAY_RATE` ‚Üí Old memories fade faster
- ‚Üë `CONFIDENCE_THRESHOLD` ‚Üí Fewer, more confident results
- ‚Üë `TOP_N_RESULTS` ‚Üí More results returned

You can experiment to find what works for your use case!

---

## üéØ Design Decisions

### Why No "Update" Command?
**Problem**: You move your passport from blue suitcase ‚Üí red backpack.

**Bad Solution**: Find and update the old memory.
- Complex: Which memory to update?
- Error-prone: What if it updates the wrong one?

**Good Solution**: Just add a new memory.
- Recency scoring naturally prioritizes the latest
- Historical record preserved (you *did* put it in blue suitcase once)
- Simple: no ambiguity

### Why Short IDs?
Full UUIDs: `550e8400-e29b-41d4-a716-446655440000` (ugly!)
Short IDs: `a3f2` (human-friendly)

We generate UUIDs internally but display only first 4 chars. Good enough for uniqueness in personal use case (16^4 = 65,536 possibilities).

### Why ChromaDB?
- ‚úÖ Simple Python API
- ‚úÖ Local persistence built-in
- ‚úÖ No server needed (embedded mode)
- ‚úÖ Fast enough for 10k+ memories
- ‚úÖ Good documentation for learners

Alternatives considered:
- **FAISS**: Faster, but no built-in persistence (more setup)
- **Qdrant**: Great but overkill for local-only use case
- **Pinecone/Weaviate**: Cloud-based (defeats our "fully local" goal)

---

## üß™ Testing Strategy

We'll test real-world scenarios:

### Test Cases

1. **Basic Add & Recall**
   - Add: "passport in blue suitcase"
   - Recall: "where's my passport?" ‚Üí should match

2. **Semantic Similarity**
   - Add: "I put my passport in the blue suitcase"
   - Recall: "where did I leave my travel document?" ‚Üí should still match

3. **Recency Prioritization**
   - Add: "passport in blue suitcase" (simulate old timestamp)
   - Add: "passport in red backpack" (now)
   - Recall: "where's my passport?" ‚Üí should return red backpack

4. **No Match Handling**
   - Query: "where's my spaceship?" (never mentioned)
   - Should return: "No confident matches found"

5. **Multiple Items**
   - Add 10 different items in different locations
   - Recall each one ‚Üí verify correct retrieval

6. **Confidence Threshold**
   - Query something vaguely related but not exact
   - Verify it respects threshold setting

---

## üöÄ Future Enhancements

### Phase 2: Conversational Interface
Add LLM layer for natural language responses:
```
Current: "You said: 'passport in red backpack' on Oct 13"
Future:  "Your passport is in the red backpack. You mentioned this yesterday evening."
```

### Phase 3: Categories/Tags
```bash
brain add "passport in backpack" --category travel
brain recall "passport" --category travel  # Only search travel memories
```

### Phase 4: Voice Interface
- Speech-to-text input (Whisper)
- Text-to-speech output

### Phase 5: Metadata Enrichment
- Auto-capture location (GPS)
- File attachments
- Photos

---

## üìñ Learning Resources

If you want to dive deeper into the concepts:

**Embeddings:**
- [Sentence Transformers Documentation](https://www.sbert.net/)
- [Word Embeddings Explained](https://jalammar.github.io/illustrated-word2vec/)

**Vector Databases:**
- [ChromaDB Docs](https://docs.trychroma.com/)
- [Understanding Vector Databases](https://www.pinecone.io/learn/vector-database/)

**RAG:**
- [What is RAG?](https://www.anthropic.com/index/retrieval-augmented-generation)
- [Building RAG Applications](https://python.langchain.com/docs/use_cases/question_answering/)

---

## üêõ Debugging Tips

### Memory not recalling correctly?
1. Check similarity scores: `brain recall --debug "query"`
2. Adjust `SIMILARITY_WEIGHT` vs `RECENCY_WEIGHT` in config
3. Lower `CONFIDENCE_THRESHOLD` to see more marginal matches

### Embeddings slow?
- First run downloads the model (~80MB)
- Subsequent runs use cached model (fast)
- CPU inference should be <100ms per encoding

### ChromaDB errors?
- Check `./data/chroma/` permissions
- Delete and reinitialize if corrupted: `rm -rf ./data/chroma/`

---

## ü§ù Contributing

This is a learning project! Feel free to:
- Experiment with different embedding models
- Try different scoring algorithms
- Add new commands
- Improve the CLI interface

Every file is heavily commented to explain *why* not just *what*.

---

**Questions?** Check the inline code comments or ask in issues!

